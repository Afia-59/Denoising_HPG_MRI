# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M2zjDyG1GqIYJFSkg3pdAsW0A3H-dvAW
"""

from google.colab import drive
drive.mount('/content/drive')

import nibabel as nib # common way of importing nibabel

mri_file = '/content/drive/MyDrive/ColabNotebooks/cleanArrayAllData.nii.gz.nii.gz'
tr_np = nib.load(mri_file)
clean=tr_np.get_fdata()
print(clean.shape)

import os
os.chdir('/content/drive/MyDrive/ColabNotebooks/Medical-Image-Denoising/')
import argparse

import cv2
import numpy as np

import dataset_reader
import samples_plt
from CNN_denoiser import CNN_denoiser
import matplotlib.pyplot as plt
from scipy.fft import fftshift
from numpy.fft import fftshift, ifftshift, fftn, ifftn

from keras import layers
from keras.models import Model
from keras.layers import LeakyReLU
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Conv2D, Conv2DTranspose
from keras.constraints import max_norm
from keras import backend as K
import matplotlib.pyplot as plt
import numpy as np
import os
from sklearn.model_selection import train_test_split
import tensorflow as tf

def normalize_images(images):
    # initial zero ndarray
    normalized_images = np.zeros_like(images.astype(float))

    # The first images index is number of images where the other indices indicates
    # hieight, width and depth of the image
    num_images = images.shape[0]

    # Computing the minimum and maximum value of the input image to do the normalization based on them
    maximum_value, minimum_value = images.max(), images.min()

    # Normalize all the pixel values of the images to be from 0 to 1
    for img in range(num_images):
        if (float(maximum_value - minimum_value) !=0 ):
          normalized_images[img, ...] = (images[img, ...] - float(minimum_value)) / float(maximum_value - minimum_value)

    return normalized_images

def quad_shift(arr):
    return np.fft.ifftshift(arr)

def noise_add_ksp(original_img):
    original_FT = np.zeros_like(original_img, dtype=np.complex128)

    for j in range(original_img.shape[2]):
        #original_FT[:, :, i] = quad_shift(np.fft.fft(np.transpose(original_img[:, :, i])))
        original_FT[:,:,j]=fftshift(fftn(original_img[:,:,j]))

    noisy_FT = np.zeros_like(original_img, dtype=np.complex128)

    for i in range(original_img.shape[2]):
        #print(i)
        #what_noise = np.random.randint(1, 3)
        what_noise = 1

        if what_noise == 1:  # Random Gaussian Noise
            #print("Gaussian")

            SD = np.random.randint(8000,20000)
            real_noise = np.random.normal(0, SD, size=(original_FT.shape[0], original_FT.shape[1]))
            img_noise = np.random.normal(0, SD, size=(original_FT.shape[0], original_FT.shape[1]))
            complex_noise = real_noise + 1j * img_noise
            noisy_FT[:, :, i] = original_FT[:, :, i] + complex_noise

        elif what_noise == 2:  # Spike Noise
            #print("spike")
            spike_no = np.random.randint(1, 50)
            random_num = np.random.rand(spike_no) + 1j * np.random.rand(spike_no)
            noisy_FT[:, :, i] = original_FT[:, :, i]

            for k in range(len(random_num)):
                index1 = np.random.randint(0, original_FT.shape[0])
                index2 = np.random.randint(0, original_FT.shape[1])
                add = np.random.randint(1,np.max(abs(original_FT[:, :, i])))
                random_num[k] += add
                noisy_FT[index1, index2, i] = random_num[k]

    noisy_img = np.zeros_like(original_img)

    for i in range(original_img.shape[2]):
         noisy_img[:, :, i] = ifftn(ifftshift(noisy_FT[:, :, i]))

    return original_FT, noisy_FT, noisy_img

def makeSlide(image_array):
    # Check if the input is a 3D array
    if len(image_array.shape) != 3:
        raise ValueError("Input should be a 3D array.")

    num_slices = image_array.shape[2]
    num_rows = int(np.ceil(np.sqrt(num_slices)))
    num_cols = int(np.ceil(num_slices / num_rows))

    plt.figure(figsize=(10, 10))

    for i in range(num_slices):
        plt.subplot(num_rows, num_cols, i + 1)
        plt.imshow(image_array[:, :, i], cmap='gray')
        plt.axis('off')
        plt.title(f"Slice {i+1}")

    plt.show()

def noiseAddinForLoop(original_img):
    oFT, nFT, mynoisy =noise_add_ksp(original_img)
    mynoisy_norm=np.zeros(shape=(500,150,150,1))
    myoriginal_norm=np.zeros(shape=(500,150,150,1))

    for j in range(mynoisy.shape[2]):
      mynoisy_norm[j,:,:,0] = normalize_images(mynoisy[:,:,j])
      myoriginal_norm[j,:,:,0] = normalize_images(original_img[:,:,j])

    return myoriginal_norm,mynoisy_norm

"""## Choose your Model. (UNet without Skip connection works better)

## UNet with Skip Connection
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Conv2DTranspose, concatenate

batch_size = 150
no_epochs = 50
validation_split = 0.2
verbosity = 1
max_norm_value = 2.0
noise_factor = 0.55
number_of_visualizations = 6
# Define the model
model = Sequential()

# Encoder
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(150, 150, 1)))
skip_connection_1 = model.layers[-1].output
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'))

# Decoder with skip connections
model.add(Conv2DTranspose(32, kernel_size=(3, 3), activation='relu', padding='same'))
skip_connection_2 = model.layers[-1].output
model.add(Conv2DTranspose(64, kernel_size=(3, 3), activation='relu', padding='same'))

# Merge skip connections
merge = concatenate([skip_connection_1, skip_connection_2], axis=-1)

# Output layer
model.add(Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same'))

# Display the model summary
model.summary()

"""## UNet without skip connection"""

# Model configuration

batch_size = 150
no_epochs = 50
validation_split = 0.2
verbosity = 1
max_norm_value = 2.0
noise_factor = 0.55
number_of_visualizations = 6

# Create the model
model = Sequential()
model.add(Conv2D(64, kernel_size=(3, 3), kernel_constraint=max_norm(max_norm_value), activation='relu', kernel_initializer='he_uniform', input_shape=(150,150,1)))
model.add(Conv2D(32, kernel_size=(3, 3), kernel_constraint=max_norm(max_norm_value), activation='relu', kernel_initializer='he_uniform'))
model.add(Conv2DTranspose(32, kernel_size=(3,3), kernel_constraint=max_norm(max_norm_value), activation='relu', kernel_initializer='he_uniform'))
model.add(Conv2DTranspose(64, kernel_size=(3,3), kernel_constraint=max_norm(max_norm_value), activation='relu', kernel_initializer='he_uniform'))
model.add(Conv2D(1, kernel_size=(3, 3), kernel_constraint=max_norm(max_norm_value), activation='sigmoid', padding='same'))

model.summary()

def ssim_loss(y_true, y_pred):
  return tf.reduce_mean(tf.image.ssim(y_true, y_pred, 2.0))

a=0
b=499

model.compile(keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',metrics=['accuracy'])
oo,nn =noiseAddinForLoop(clean[:,:,a:b])
xtrain, xtest, ytrain, ytest = train_test_split(nn,oo, test_size=0.1)
model.fit(xtrain, ytrain,epochs=1,verbose=1)
myWeights=model.get_weights()

a=0
b=499
LR=0.01

for part in range(1,10):
  print('a',a,'b',b)
  oo,nn =noiseAddinForLoop(clean[:,:,a:b])
  xtrain, xtest, ytrain, ytest = train_test_split(nn,oo, test_size=0.1)
  model.set_weights(myWeights)
  model.compile(keras.optimizers.Adam(learning_rate=LR), loss='binary_crossentropy',metrics=['accuracy'])
  model.fit(xtrain, ytrain,epochs=10,verbose=1)
  myWeights=model.get_weights()
  a=b+1
  b=b+499

  if  (b>clean.shape[2]):
    num_slices = clean.shape[2]
    all_slices = np.arange(num_slices)
    np.random.shuffle(all_slices)
    clean_shuffle=np.zeros(shape=(clean.shape[0],clean.shape[1],clean.shape[2]))
    # Now you can use 'selected_slices' to access the 3D slices you need
    for i in range(clean.shape[2]):
          #print(i)
          clean_shuffle[:,:,i] = clean[:, :, all_slices[i]]
    clean=clean_shuffle
    a=0
    b=499
    LR=LR/10

"""## Save your model"""

model.save("/content/drive/MyDrive/Colab Notebooks/Change_trainData_in_each_iteration_NoSkipConn.h5")

"""## Predictions"""

predictions = model.predict(xtest)
predictions_3d=np.zeros(shape=(150,150,predictions.shape[2]))

for j in range(predictions.shape[0]):
   predictions_3d[:,:,j] = (predictions[j,:,:,0])

makeSlide(predictions_3d[:,:,0:49])

"""## Display"""

f = plt.figure(figsize=(20,8))

s1=48
s2=31
s3=2
s4=9
s5=11

f.add_subplot(5,3,1)
plt.imshow(ytest[s1,:,:,0],cmap='Greys_r')
plt.title("True")

f.add_subplot(5,3,2)
plt.imshow(xtest[s1,:,:,0],cmap='Greys_r')
plt.title("Noisy")

f.add_subplot(5,3,3)
plt.imshow(predictions[s1,:,:,0],cmap='Greys_r',vmin=0,vmax=1)
plt.title("CNN")

###############################################
f.add_subplot(5,3,4)
plt.imshow(ytest[s2,:,:,0],cmap='Greys_r')
plt.title("True")

f.add_subplot(5,3,5)
plt.imshow(xtest[s2,:,:,0],cmap='Greys_r')
plt.title("Noisy")

f.add_subplot(5,3,6)
plt.imshow(predictions[s2,:,:,0],cmap='Greys_r',vmin=0,vmax=1)
plt.title("CNN")

###############################################
f.add_subplot(5,3,7)
plt.imshow(ytest[s3,:,:,0],cmap='Greys_r')
plt.title("True")

f.add_subplot(5,3,8)
plt.imshow(xtest[s3,:,:,0],cmap='Greys_r')
plt.title("Noisy")

f.add_subplot(5,3,9)
plt.imshow(predictions[s3,:,:,0],cmap='Greys_r',vmin=0,vmax=1)
plt.title("CNN")

###############################################

f.add_subplot(5,3,10)
plt.imshow(ytest[s4,:,:,0],cmap='Greys_r')
plt.title("True")

f.add_subplot(5,3,11)
plt.imshow(xtest[s4,:,:,0],cmap='Greys_r')
plt.title("Noisy")

f.add_subplot(5,3,12)
plt.imshow(predictions[s4,:,:,0],cmap='Greys_r',vmin=0,vmax=1)
plt.title("CNN")

###############################################

f.add_subplot(5,3,13)
plt.imshow(ytest[s5,:,:,0],cmap='Greys_r')
plt.title("True")

f.add_subplot(5,3,14)
plt.imshow(xtest[s5,:,:,0],cmap='Greys_r')
plt.title("Noisy")

f.add_subplot(5,3,15)
plt.imshow(predictions[s5,:,:,0],cmap='Greys_r',vmin=0,vmax=1)
plt.title("CNN")

plt.subplots_adjust(wspace=0, hspace=0, left=0, right=.5, bottom=0, top=1)